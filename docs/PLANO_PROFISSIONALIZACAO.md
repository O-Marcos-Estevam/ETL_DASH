# ğŸš€ Plano de ProfissionalizaÃ§Ã£o - ETL_DASH

## VisÃ£o Geral

Este documento detalha todos os passos necessÃ¡rios para transformar o ETL_DASH em um **sistema de produÃ§Ã£o profissional**, seguindo as melhores prÃ¡ticas da indÃºstria.

**Estimativa Total**: 8-12 semanas
**Prioridade**: SeguranÃ§a â†’ Escalabilidade â†’ Qualidade â†’ DevOps

---

## ğŸ“Š Resumo das Fases

| Fase | DescriÃ§Ã£o | Prioridade | Estimativa |
|------|-----------|------------|------------|
| 1 | SeguranÃ§a | ğŸ”´ CrÃ­tica | 1-2 semanas |
| 2 | Banco de Dados | ğŸ”´ CrÃ­tica | 1 semana |
| 3 | Arquitetura | ğŸŸ  Alta | 1-2 semanas |
| 4 | Qualidade (Testes) | ğŸŸ  Alta | 2 semanas |
| 5 | Escalabilidade | ğŸŸ¡ MÃ©dia | 1-2 semanas |
| 6 | Observabilidade | ğŸŸ¡ MÃ©dia | 1 semana |
| 7 | DevOps | ğŸŸ¡ MÃ©dia | 1 semana |
| 8 | DocumentaÃ§Ã£o | ğŸŸ¢ Normal | 3-5 dias |

---

# FASE 1: SeguranÃ§a ğŸ”

## 1.1 AutenticaÃ§Ã£o JWT

### Objetivo
Implementar autenticaÃ§Ã£o baseada em JWT (JSON Web Tokens) para proteger todos os endpoints da API.

### Passos

#### 1.1.1 Criar mÃ³dulo de autenticaÃ§Ã£o
```
backend/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ auth.py          # NOVO: LÃ³gica de autenticaÃ§Ã£o
â”‚   â”œâ”€â”€ security.py      # NOVO: FunÃ§Ãµes de criptografia
â”‚   â””â”€â”€ dependencies.py  # NOVO: DependÃªncias FastAPI
```

**Arquivo: `backend/core/auth.py`**
- Implementar `create_access_token(data: dict) -> str`
- Implementar `verify_token(token: str) -> TokenPayload`
- Implementar `get_password_hash(password: str) -> str`
- Implementar `verify_password(plain: str, hashed: str) -> bool`

**Bibliotecas necessÃ¡rias:**
```
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
```

#### 1.1.2 Criar modelo de usuÃ¡rio
```python
# backend/models/user.py
class User(BaseModel):
    id: int
    username: str
    email: str
    hashed_password: str
    is_active: bool = True
    is_admin: bool = False
    created_at: datetime
```

#### 1.1.3 Criar endpoints de autenticaÃ§Ã£o
```
POST /api/auth/login      â†’ Retorna access_token + refresh_token
POST /api/auth/refresh    â†’ Renova access_token
POST /api/auth/logout     â†’ Invalida tokens
GET  /api/auth/me         â†’ Retorna usuÃ¡rio atual
```

#### 1.1.4 Proteger rotas existentes
```python
# Exemplo de proteÃ§Ã£o
from core.dependencies import get_current_user

@router.post("/execute")
async def execute_pipeline(
    request: ExecuteRequest,
    current_user: User = Depends(get_current_user)  # Requer auth
):
    ...
```

#### 1.1.5 Atualizar frontend
- Criar pÃ¡gina de login (`/login`)
- Armazenar token em httpOnly cookie (mais seguro que localStorage)
- Implementar interceptor Axios para incluir token
- Implementar refresh automÃ¡tico de token
- Redirecionar para login quando 401

### EntregÃ¡veis
- [ ] MÃ³dulo `core/auth.py` com JWT
- [ ] Modelo `User` com senha hash
- [ ] Endpoints `/api/auth/*`
- [ ] Middleware de autenticaÃ§Ã£o
- [ ] PÃ¡gina de login no frontend
- [ ] Interceptor Axios com token

---

## 1.2 Criptografia de Credenciais

### Objetivo
Criptografar credenciais em repouso usando AES-256.

### Passos

#### 1.2.1 Implementar serviÃ§o de criptografia
```python
# backend/core/crypto.py
from cryptography.fernet import Fernet

class CryptoService:
    def __init__(self, key: bytes):
        self.fernet = Fernet(key)

    def encrypt(self, data: str) -> str:
        return self.fernet.encrypt(data.encode()).decode()

    def decrypt(self, encrypted: str) -> str:
        return self.fernet.decrypt(encrypted.encode()).decode()
```

#### 1.2.2 Gerar e armazenar chave mestra
- Gerar chave com `Fernet.generate_key()`
- Armazenar em variÃ¡vel de ambiente `ETL_ENCRYPTION_KEY`
- Alternativa: usar AWS KMS, Azure Key Vault, ou HashiCorp Vault

#### 1.2.3 Migrar credentials.json
```python
# Estrutura atual (INSEGURO)
{
    "amplis": {
        "username": "user",
        "password": "plain_text_password"  # âŒ
    }
}

# Estrutura nova (SEGURO)
{
    "amplis": {
        "username": "user",
        "password": "gAAAAABf..."  # âœ… Criptografado
    }
}
```

#### 1.2.4 Atualizar CredentialsService
- Descriptografar ao carregar
- Criptografar ao salvar
- Nunca logar senhas em texto plano

### EntregÃ¡veis
- [ ] MÃ³dulo `core/crypto.py`
- [ ] Script de migraÃ§Ã£o de credenciais existentes
- [ ] `CredentialsService` atualizado
- [ ] DocumentaÃ§Ã£o de gerenciamento de chaves

---

## 1.3 ValidaÃ§Ã£o de Entrada

### Objetivo
Validar todas as entradas do usuÃ¡rio para prevenir injeÃ§Ã£o e dados invÃ¡lidos.

### Passos

#### 1.3.1 Criar schemas Pydantic para todas as requests
```python
# backend/schemas/execution.py
from pydantic import BaseModel, Field, validator
from datetime import date

class ExecuteRequest(BaseModel):
    sistemas: list[str] = Field(..., min_items=1)
    data_inicial: date
    data_final: date
    limpar: bool = False

    @validator('sistemas')
    def validate_sistemas(cls, v):
        allowed = {'amplis_reag', 'amplis_master', 'maps', ...}
        for s in v:
            if s not in allowed:
                raise ValueError(f'Sistema invÃ¡lido: {s}')
        return v

    @validator('data_final')
    def validate_dates(cls, v, values):
        if 'data_inicial' in values and v < values['data_inicial']:
            raise ValueError('data_final deve ser >= data_inicial')
        return v
```

#### 1.3.2 Sanitizar logs
- Remover credenciais de mensagens de log
- Mascarar dados sensÃ­veis (CPF, CNPJ, etc.)

### EntregÃ¡veis
- [ ] Schemas Pydantic para todos os endpoints
- [ ] Validadores customizados
- [ ] SanitizaÃ§Ã£o de logs
- [ ] Testes de validaÃ§Ã£o

---

## 1.4 Rate Limiting

### Objetivo
Proteger API contra abuso e ataques de forÃ§a bruta.

### Passos

#### 1.4.1 Implementar rate limiting
```python
# Usar slowapi
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@app.post("/api/auth/login")
@limiter.limit("5/minute")  # 5 tentativas por minuto
async def login(...):
    ...

@app.post("/api/execute")
@limiter.limit("10/minute")  # 10 execuÃ§Ãµes por minuto
async def execute(...):
    ...
```

### EntregÃ¡veis
- [ ] Rate limiting configurado
- [ ] Limites por endpoint
- [ ] Headers de rate limit na resposta

---

## 1.5 CORS e Headers de SeguranÃ§a

### Passos

#### 1.5.1 Restringir CORS
```python
# Apenas origens permitidas
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://etl.empresa.com"],  # EspecÃ­fico
    allow_credentials=True,
    allow_methods=["GET", "POST", "PATCH", "DELETE"],
    allow_headers=["Authorization", "Content-Type"],
)
```

#### 1.5.2 Adicionar headers de seguranÃ§a
```python
@app.middleware("http")
async def add_security_headers(request, call_next):
    response = await call_next(request)
    response.headers["X-Content-Type-Options"] = "nosniff"
    response.headers["X-Frame-Options"] = "DENY"
    response.headers["X-XSS-Protection"] = "1; mode=block"
    response.headers["Strict-Transport-Security"] = "max-age=31536000"
    return response
```

### EntregÃ¡veis
- [ ] CORS restritivo
- [ ] Security headers
- [ ] CSP (Content Security Policy)

---

# FASE 2: Banco de Dados ğŸ—„ï¸

## 2.1 MigraÃ§Ã£o SQLite â†’ PostgreSQL

### Objetivo
Substituir SQLite por PostgreSQL para suportar concorrÃªncia e escala.

### Passos

#### 2.1.1 Configurar PostgreSQL
```yaml
# docker-compose.yml
services:
  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_DB: etl_dash
      POSTGRES_USER: etl_user
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
```

#### 2.1.2 Implementar SQLAlchemy ORM
```python
# backend/core/database.py
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import declarative_base

DATABASE_URL = os.getenv("DATABASE_URL", "postgresql+asyncpg://...")

engine = create_async_engine(DATABASE_URL, echo=False)
Base = declarative_base()

async def get_db() -> AsyncSession:
    async with AsyncSession(engine) as session:
        yield session
```

#### 2.1.3 Criar modelos ORM
```python
# backend/models/job.py
from sqlalchemy import Column, Integer, String, DateTime, JSON, Enum
from core.database import Base

class Job(Base):
    __tablename__ = "jobs"

    id = Column(Integer, primary_key=True, index=True)
    type = Column(String(50), nullable=False)
    status = Column(Enum(JobStatus), default=JobStatus.PENDING)
    sistemas = Column(JSON, nullable=False)
    params = Column(JSON)
    logs = Column(Text)
    error_message = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)
    started_at = Column(DateTime)
    finished_at = Column(DateTime)
    created_by = Column(Integer, ForeignKey("users.id"))
```

#### 2.1.4 Implementar Alembic para migraÃ§Ãµes
```bash
# Estrutura
backend/
â”œâ”€â”€ alembic/
â”‚   â”œâ”€â”€ versions/
â”‚   â”‚   â”œâ”€â”€ 001_initial.py
â”‚   â”‚   â”œâ”€â”€ 002_add_users.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ env.py
â”‚   â””â”€â”€ alembic.ini
```

```bash
# Comandos
alembic revision --autogenerate -m "initial"
alembic upgrade head
```

#### 2.1.5 Script de migraÃ§Ã£o de dados
```python
# scripts/migrate_sqlite_to_postgres.py
# Migrar jobs existentes do SQLite para PostgreSQL
```

### EntregÃ¡veis
- [ ] PostgreSQL configurado (Docker)
- [ ] SQLAlchemy ORM implementado
- [ ] Modelos ORM para todas as entidades
- [ ] Alembic configurado
- [ ] MigraÃ§Ãµes iniciais
- [ ] Script de migraÃ§Ã£o de dados
- [ ] Connection pooling configurado

---

## 2.2 Modelo de Dados Expandido

### Novos modelos

```
users           â†’ UsuÃ¡rios do sistema
jobs            â†’ Fila de execuÃ§Ã£o (expandido)
job_logs        â†’ Logs separados por job
systems         â†’ ConfiguraÃ§Ã£o de sistemas
credentials     â†’ Credenciais (criptografadas)
audit_logs      â†’ Trilha de auditoria
```

### Schema completo

```sql
-- users
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    hashed_password VARCHAR(255) NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    is_admin BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT NOW(),
    last_login TIMESTAMP
);

-- jobs (expandido)
CREATE TABLE jobs (
    id SERIAL PRIMARY KEY,
    type VARCHAR(50) NOT NULL,
    status VARCHAR(20) NOT NULL DEFAULT 'pending',
    priority INTEGER DEFAULT 0,
    sistemas JSONB NOT NULL,
    params JSONB,
    result JSONB,
    error_message TEXT,
    retry_count INTEGER DEFAULT 0,
    max_retries INTEGER DEFAULT 3,
    created_at TIMESTAMP DEFAULT NOW(),
    started_at TIMESTAMP,
    finished_at TIMESTAMP,
    created_by INTEGER REFERENCES users(id),

    INDEX idx_jobs_status (status),
    INDEX idx_jobs_created_at (created_at)
);

-- job_logs (separado para performance)
CREATE TABLE job_logs (
    id SERIAL PRIMARY KEY,
    job_id INTEGER REFERENCES jobs(id) ON DELETE CASCADE,
    level VARCHAR(10) NOT NULL,
    sistema VARCHAR(50),
    message TEXT NOT NULL,
    timestamp TIMESTAMP DEFAULT NOW(),

    INDEX idx_job_logs_job_id (job_id),
    INDEX idx_job_logs_timestamp (timestamp)
);

-- audit_logs
CREATE TABLE audit_logs (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(id),
    action VARCHAR(100) NOT NULL,
    resource_type VARCHAR(50),
    resource_id INTEGER,
    details JSONB,
    ip_address INET,
    created_at TIMESTAMP DEFAULT NOW()
);
```

### EntregÃ¡veis
- [ ] Schema SQL completo
- [ ] Modelos SQLAlchemy
- [ ] Ãndices otimizados
- [ ] Relacionamentos definidos

---

# FASE 3: Arquitetura ğŸ—ï¸

## 3.1 Dependency Injection

### Objetivo
Implementar injeÃ§Ã£o de dependÃªncias para melhor testabilidade e desacoplamento.

### Passos

#### 3.1.1 Criar container de dependÃªncias
```python
# backend/core/container.py
from dependency_injector import containers, providers

class Container(containers.DeclarativeContainer):
    config = providers.Configuration()

    # Database
    db_engine = providers.Singleton(
        create_async_engine,
        config.database_url
    )

    # Services
    credentials_service = providers.Factory(
        CredentialsService,
        crypto=providers.Dependency()
    )

    sistema_service = providers.Singleton(SistemaService)

    executor_service = providers.Factory(
        ETLExecutor,
        sistema_service=sistema_service
    )

    worker_service = providers.Singleton(
        BackgroundWorker,
        executor=executor_service
    )
```

#### 3.1.2 Integrar com FastAPI
```python
# backend/app.py
from core.container import Container

container = Container()
container.config.from_dict(settings.dict())

app = FastAPI()
app.container = container

# Usar em rotas
@router.post("/execute")
async def execute(
    request: ExecuteRequest,
    executor: ETLExecutor = Depends(Provide[Container.executor_service])
):
    ...
```

### EntregÃ¡veis
- [ ] Container de DI configurado
- [ ] Todos os services refatorados
- [ ] Testes usando mocks via DI

---

## 3.2 Consolidar ConfiguraÃ§Ã£o

### Objetivo
Centralizar toda configuraÃ§Ã£o em um Ãºnico mÃ³dulo.

### Passos

#### 3.2.1 Criar Settings com Pydantic
```python
# backend/core/settings.py
from pydantic_settings import BaseSettings
from functools import lru_cache

class Settings(BaseSettings):
    # Server
    host: str = "0.0.0.0"
    port: int = 4001
    debug: bool = False

    # Database
    database_url: str = "postgresql+asyncpg://..."

    # Security
    secret_key: str
    encryption_key: str
    access_token_expire_minutes: int = 30

    # Redis
    redis_url: str = "redis://localhost:6379"

    # Execution
    job_timeout: int = 3600
    max_retries: int = 3

    # Paths
    config_dir: Path = Path("config")
    logs_dir: Path = Path("logs")

    class Config:
        env_file = ".env"
        env_prefix = "ETL_"

@lru_cache
def get_settings() -> Settings:
    return Settings()
```

#### 3.2.2 Remover paths hardcoded
- Usar `settings.config_dir` em vez de `os.path.dirname(__file__)`
- Injetar paths via configuraÃ§Ã£o

### EntregÃ¡veis
- [ ] `Settings` centralizado
- [ ] Arquivo `.env.example` atualizado
- [ ] Paths injetados via config
- [ ] ValidaÃ§Ã£o de configuraÃ§Ã£o no startup

---

## 3.3 Estado Global â†’ Gerenciado

### Objetivo
Eliminar variÃ¡veis globais e centralizar estado.

### Passos

#### 3.3.1 Backend - Eliminar state.py
```python
# ANTES (state.py)
ws_manager = None  # Global mutÃ¡vel

# DEPOIS (via DI)
class AppState:
    def __init__(self):
        self.ws_manager = ConnectionManager()
        self.worker = None

    async def startup(self):
        self.worker = BackgroundWorker(...)
        await self.worker.start()

    async def shutdown(self):
        await self.worker.stop()
```

#### 3.3.2 Frontend - Implementar Zustand
```typescript
// frontend/src/stores/etl-store.ts
import { create } from 'zustand'
import { devtools, persist } from 'zustand/middleware'

interface ETLState {
    // Estado
    config: ConfiguracaoETL | null
    sistemas: Sistema[]
    currentJob: Job | null
    logs: LogEntry[]
    isExecuting: boolean
    isConnected: boolean

    // Actions
    setConfig: (config: ConfiguracaoETL) => void
    updateSistema: (id: string, updates: Partial<Sistema>) => void
    addLog: (log: LogEntry) => void
    clearLogs: () => void
    setExecuting: (value: boolean) => void
}

export const useETLStore = create<ETLState>()(
    devtools(
        persist(
            (set) => ({
                config: null,
                sistemas: [],
                currentJob: null,
                logs: [],
                isExecuting: false,
                isConnected: false,

                setConfig: (config) => set({ config }),
                updateSistema: (id, updates) => set((state) => ({
                    sistemas: state.sistemas.map(s =>
                        s.id === id ? { ...s, ...updates } : s
                    )
                })),
                addLog: (log) => set((state) => ({
                    logs: [...state.logs.slice(-999), log]  // Max 1000 logs
                })),
                clearLogs: () => set({ logs: [] }),
                setExecuting: (value) => set({ isExecuting: value })
            }),
            { name: 'etl-storage' }
        )
    )
)
```

### EntregÃ¡veis
- [ ] `state.py` eliminado
- [ ] `AppState` implementado
- [ ] Zustand store criado
- [ ] Componentes usando store

---

## 3.4 Separar Logs em Tabela PrÃ³pria

### Objetivo
Melhorar performance separando logs dos jobs.

### Passos

```python
# ANTES: logs concatenados no job
UPDATE jobs SET logs = logs || 'nova linha'

# DEPOIS: tabela separada
INSERT INTO job_logs (job_id, level, sistema, message) VALUES (...)
```

### EntregÃ¡veis
- [ ] Tabela `job_logs` criada
- [ ] `LogRepository` implementado
- [ ] Queries otimizadas com paginaÃ§Ã£o

---

# FASE 4: Qualidade ğŸ§ª

## 4.1 Testes UnitÃ¡rios Backend

### Objetivo
Atingir 80%+ de cobertura de cÃ³digo.

### Estrutura de testes
```
backend/tests/
â”œâ”€â”€ conftest.py              # Fixtures globais
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_auth.py         # Testes de autenticaÃ§Ã£o
â”‚   â”œâ”€â”€ test_crypto.py       # Testes de criptografia
â”‚   â”œâ”€â”€ test_executor.py     # Testes do executor
â”‚   â”œâ”€â”€ test_worker.py       # Testes do worker
â”‚   â”œâ”€â”€ test_credentials.py  # Testes de credenciais
â”‚   â”œâ”€â”€ test_sistemas.py     # Testes de sistemas
â”‚   â””â”€â”€ test_models.py       # Testes de modelos
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_api_auth.py     # Testes de API auth
â”‚   â”œâ”€â”€ test_api_execute.py  # Testes de API execute
â”‚   â”œâ”€â”€ test_api_config.py   # Testes de API config
â”‚   â”œâ”€â”€ test_websocket.py    # Testes WebSocket
â”‚   â””â”€â”€ test_database.py     # Testes de banco
â””â”€â”€ e2e/
    â””â”€â”€ test_full_pipeline.py # Teste end-to-end
```

### Ferramentas
```
pytest==8.3.4
pytest-asyncio==0.24.0
pytest-cov==4.1.0
pytest-mock==3.14.0
httpx==0.27.0          # Async HTTP client para testes
factory-boy==3.3.0     # Factories para modelos
faker==24.0.0          # Dados fake
```

### Meta de cobertura
```bash
pytest --cov=backend --cov-report=html --cov-fail-under=80
```

### EntregÃ¡veis
- [ ] 80%+ cobertura
- [ ] Testes de auth
- [ ] Testes de API (integration)
- [ ] Testes de WebSocket
- [ ] Testes de database
- [ ] CI rodando testes

---

## 4.2 Testes Frontend

### Estrutura
```
frontend/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ __tests__/           # Testes unitÃ¡rios
â”‚       â”œâ”€â”€ components/
â”‚       â”œâ”€â”€ hooks/
â”‚       â”œâ”€â”€ stores/
â”‚       â””â”€â”€ utils/
â”œâ”€â”€ e2e/                     # Testes E2E
â”‚   â”œâ”€â”€ auth.spec.ts
â”‚   â”œâ”€â”€ etl.spec.ts
â”‚   â””â”€â”€ settings.spec.ts
â”œâ”€â”€ vitest.config.ts
â””â”€â”€ playwright.config.ts
```

### Ferramentas
```json
{
  "devDependencies": {
    "@testing-library/react": "^14.0.0",
    "@testing-library/jest-dom": "^6.0.0",
    "@testing-library/user-event": "^14.0.0",
    "vitest": "^1.0.0",
    "@vitest/coverage-v8": "^1.0.0",
    "msw": "^2.0.0",
    "playwright": "^1.40.0"
  }
}
```

### EntregÃ¡veis
- [ ] Vitest configurado
- [ ] Testes de componentes
- [ ] Testes de hooks
- [ ] Testes de store
- [ ] Playwright E2E
- [ ] 70%+ cobertura frontend

---

## 4.3 CI/CD Pipeline

### GitHub Actions

```yaml
# .github/workflows/ci.yml
name: CI

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  backend-tests:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_PASSWORD: test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -r backend/requirements-dev.txt
      - run: pytest backend/tests --cov --cov-fail-under=80

  frontend-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
      - run: cd frontend && npm ci
      - run: cd frontend && npm run test -- --coverage
      - run: cd frontend && npm run build

  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - run: pip install ruff
      - run: ruff check backend/
      - run: cd frontend && npm ci && npm run lint

  security:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - run: pip install bandit safety
      - run: bandit -r backend/
      - run: safety check -r backend/requirements.txt
```

### EntregÃ¡veis
- [ ] CI configurado
- [ ] Testes automatizados
- [ ] Linting automatizado
- [ ] Security scanning
- [ ] Build automÃ¡tico

---

## 4.4 Linting e FormataÃ§Ã£o

### Backend
```toml
# pyproject.toml
[tool.ruff]
line-length = 100
select = ["E", "F", "W", "I", "N", "S", "B"]

[tool.black]
line-length = 100

[tool.mypy]
python_version = "3.11"
strict = true
```

### Frontend
```json
// .eslintrc.json
{
  "extends": [
    "eslint:recommended",
    "plugin:@typescript-eslint/recommended",
    "plugin:react-hooks/recommended",
    "prettier"
  ]
}
```

### Pre-commit hooks
```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.0
    hooks:
      - id: ruff
      - id: ruff-format
  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.0.0
    hooks:
      - id: mypy
```

### EntregÃ¡veis
- [ ] Ruff + Black configurados
- [ ] ESLint + Prettier configurados
- [ ] MyPy para type checking
- [ ] Pre-commit hooks
- [ ] Editor configs (.editorconfig)

---

# FASE 5: Escalabilidade ğŸ“ˆ

## 5.1 Message Queue (Celery + Redis)

### Objetivo
Permitir execuÃ§Ã£o paralela de jobs e melhor gerenciamento de filas.

### Arquitetura
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI    â”‚â”€â”€â”€â”€â–¶â”‚  Redis  â”‚â”€â”€â”€â”€â–¶â”‚ Celery       â”‚
â”‚  (Producer) â”‚     â”‚ (Broker)â”‚     â”‚ Workers (N)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ImplementaÃ§Ã£o

```python
# backend/core/celery.py
from celery import Celery

celery_app = Celery(
    "etl_dash",
    broker=os.getenv("CELERY_BROKER_URL", "redis://localhost:6379/0"),
    backend=os.getenv("CELERY_RESULT_BACKEND", "redis://localhost:6379/1"),
)

celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="America/Sao_Paulo",
    task_track_started=True,
    task_time_limit=3600,
    worker_prefetch_multiplier=1,
    worker_concurrency=4,
)
```

```python
# backend/tasks/etl.py
from core.celery import celery_app

@celery_app.task(bind=True, max_retries=3)
def execute_pipeline(self, job_id: int, sistemas: list, params: dict):
    try:
        # Executar ETL
        executor = ETLExecutor()
        result = executor.execute(sistemas, params)
        return {"status": "success", "result": result}
    except Exception as e:
        self.retry(exc=e, countdown=60)
```

```python
# Endpoint atualizado
@router.post("/execute")
async def execute(request: ExecuteRequest):
    job = await create_job(request)
    task = execute_pipeline.delay(job.id, request.sistemas, request.params)
    return {"job_id": job.id, "task_id": task.id}
```

### Docker Compose
```yaml
services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  celery-worker:
    build: ./backend
    command: celery -A core.celery worker -l info -c 4
    depends_on:
      - redis
      - postgres
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0

  celery-beat:
    build: ./backend
    command: celery -A core.celery beat -l info
    depends_on:
      - redis
```

### EntregÃ¡veis
- [ ] Celery configurado
- [ ] Tasks de ETL
- [ ] Workers escalÃ¡veis
- [ ] Monitoramento (Flower)
- [ ] Retry automÃ¡tico

---

## 5.2 Redis para WebSocket Pub/Sub

### Objetivo
Permitir broadcast entre mÃºltiplas instÃ¢ncias do backend.

### ImplementaÃ§Ã£o
```python
# backend/core/pubsub.py
import aioredis

class RedisPubSub:
    def __init__(self, redis_url: str):
        self.redis = aioredis.from_url(redis_url)
        self.pubsub = self.redis.pubsub()

    async def publish(self, channel: str, message: dict):
        await self.redis.publish(channel, json.dumps(message))

    async def subscribe(self, channel: str, callback):
        await self.pubsub.subscribe(channel)
        async for message in self.pubsub.listen():
            if message["type"] == "message":
                await callback(json.loads(message["data"]))
```

```python
# backend/app.py
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        # Inscrever no Redis
        await pubsub.subscribe("etl:logs",
            lambda msg: websocket.send_json(msg))

        while True:
            await websocket.receive_text()
    except WebSocketDisconnect:
        manager.disconnect(websocket)
```

### EntregÃ¡veis
- [ ] Redis pub/sub implementado
- [ ] WebSocket usando Redis
- [ ] MÃºltiplas instÃ¢ncias suportadas

---

## 5.3 Cache com Redis

### ImplementaÃ§Ã£o
```python
# backend/core/cache.py
from functools import wraps
import aioredis

redis = aioredis.from_url(settings.redis_url)

def cached(ttl: int = 300):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            key = f"{func.__name__}:{hash(str(args) + str(kwargs))}"

            cached = await redis.get(key)
            if cached:
                return json.loads(cached)

            result = await func(*args, **kwargs)
            await redis.setex(key, ttl, json.dumps(result))
            return result
        return wrapper
    return decorator

# Uso
@cached(ttl=60)
async def get_sistemas():
    return await db.query(Sistema).all()
```

### EntregÃ¡veis
- [ ] Cache layer implementado
- [ ] InvalidaÃ§Ã£o de cache
- [ ] Cache de configuraÃ§Ã£o
- [ ] Cache de sistemas

---

# FASE 6: Observabilidade ğŸ“Š

## 6.1 Logging Estruturado

### ImplementaÃ§Ã£o
```python
# backend/core/logging.py
import structlog

structlog.configure(
    processors=[
        structlog.contextvars.merge_contextvars,
        structlog.processors.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ],
    wrapper_class=structlog.make_filtering_bound_logger(logging.INFO),
)

log = structlog.get_logger()

# Uso
log.info("job_started", job_id=123, sistemas=["maps", "qore"])
```

### Output
```json
{
  "event": "job_started",
  "job_id": 123,
  "sistemas": ["maps", "qore"],
  "level": "info",
  "timestamp": "2024-01-15T10:30:00Z",
  "request_id": "abc-123"
}
```

### EntregÃ¡veis
- [ ] structlog configurado
- [ ] Request ID em todos os logs
- [ ] Logs JSON para ELK/Loki
- [ ] Log rotation configurado

---

## 6.2 MÃ©tricas (Prometheus)

### ImplementaÃ§Ã£o
```python
# backend/core/metrics.py
from prometheus_client import Counter, Histogram, Gauge

# MÃ©tricas
jobs_total = Counter(
    "etl_jobs_total",
    "Total de jobs executados",
    ["sistema", "status"]
)

job_duration = Histogram(
    "etl_job_duration_seconds",
    "DuraÃ§Ã£o dos jobs em segundos",
    ["sistema"]
)

active_connections = Gauge(
    "etl_websocket_connections",
    "ConexÃµes WebSocket ativas"
)

# Uso
jobs_total.labels(sistema="maps", status="success").inc()
job_duration.labels(sistema="maps").observe(45.2)
```

### Endpoint
```python
from prometheus_client import make_asgi_app

app.mount("/metrics", make_asgi_app())
```

### EntregÃ¡veis
- [ ] MÃ©tricas de jobs
- [ ] MÃ©tricas de API (latÃªncia, erros)
- [ ] MÃ©tricas de WebSocket
- [ ] Dashboard Grafana

---

## 6.3 Health Checks

### ImplementaÃ§Ã£o
```python
# backend/routers/health.py
@router.get("/health")
async def health():
    return {"status": "ok"}

@router.get("/health/ready")
async def readiness():
    checks = {
        "database": await check_database(),
        "redis": await check_redis(),
        "celery": await check_celery(),
    }

    all_healthy = all(c["status"] == "ok" for c in checks.values())
    status_code = 200 if all_healthy else 503

    return JSONResponse(
        content={"status": "ready" if all_healthy else "not_ready", "checks": checks},
        status_code=status_code
    )

@router.get("/health/live")
async def liveness():
    return {"status": "alive"}
```

### EntregÃ¡veis
- [ ] `/health` bÃ¡sico
- [ ] `/health/ready` com checks
- [ ] `/health/live` para k8s
- [ ] Alertas configurados

---

## 6.4 Tracing (OpenTelemetry)

### ImplementaÃ§Ã£o
```python
# backend/core/tracing.py
from opentelemetry import trace
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

# Configurar
tracer = trace.get_tracer(__name__)

# Instrumentar FastAPI
FastAPIInstrumentor.instrument_app(app)

# Uso manual
with tracer.start_as_current_span("execute_etl") as span:
    span.set_attribute("job_id", job_id)
    span.set_attribute("sistemas", sistemas)
    result = await executor.execute(...)
```

### EntregÃ¡veis
- [ ] OpenTelemetry configurado
- [ ] Auto-instrumentaÃ§Ã£o
- [ ] Traces distribuÃ­dos
- [ ] Jaeger/Tempo para visualizaÃ§Ã£o

---

# FASE 7: DevOps ğŸ³

## 7.1 ContainerizaÃ§Ã£o

### Dockerfile Backend
```dockerfile
# backend/Dockerfile
FROM python:3.11-slim

WORKDIR /app

# DependÃªncias do sistema
RUN apt-get update && apt-get install -y \
    chromium \
    chromium-driver \
    && rm -rf /var/lib/apt/lists/*

# DependÃªncias Python
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# CÃ³digo
COPY . .

# UsuÃ¡rio nÃ£o-root
RUN useradd -m appuser && chown -R appuser:appuser /app
USER appuser

EXPOSE 4001

CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "4001"]
```

### Dockerfile Frontend
```dockerfile
# frontend/Dockerfile
FROM node:20-alpine AS builder
WORKDIR /app
COPY package*.json ./
RUN npm ci
COPY . .
RUN npm run build

FROM nginx:alpine
COPY --from=builder /app/dist /usr/share/nginx/html
COPY nginx.conf /etc/nginx/conf.d/default.conf
EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]
```

### EntregÃ¡veis
- [ ] Dockerfile backend
- [ ] Dockerfile frontend
- [ ] .dockerignore
- [ ] Multi-stage builds
- [ ] Security scanning (Trivy)

---

## 7.2 Docker Compose Completo

```yaml
# docker-compose.yml
version: '3.8'

services:
  # Banco de dados
  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_DB: etl_dash
      POSTGRES_USER: etl
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U etl"]
      interval: 5s
      timeout: 5s
      retries: 5

  # Cache e Message Broker
  redis:
    image: redis:7-alpine
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s

  # Backend API
  backend:
    build: ./backend
    environment:
      - DATABASE_URL=postgresql+asyncpg://etl:${DB_PASSWORD}@postgres/etl_dash
      - REDIS_URL=redis://redis:6379
      - ETL_SECRET_KEY=${SECRET_KEY}
      - ETL_ENCRYPTION_KEY=${ENCRYPTION_KEY}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    ports:
      - "4001:4001"

  # Celery Worker
  celery-worker:
    build: ./backend
    command: celery -A core.celery worker -l info -c 4
    environment:
      - DATABASE_URL=postgresql+asyncpg://etl:${DB_PASSWORD}@postgres/etl_dash
      - CELERY_BROKER_URL=redis://redis:6379/0
    depends_on:
      - backend
      - redis
    deploy:
      replicas: 2

  # Frontend
  frontend:
    build: ./frontend
    ports:
      - "4000:80"
    depends_on:
      - backend

  # Monitoramento
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"

  grafana:
    image: grafana/grafana:latest
    volumes:
      - grafana_data:/var/lib/grafana
    ports:
      - "3000:3000"

volumes:
  postgres_data:
  redis_data:
  grafana_data:
```

### EntregÃ¡veis
- [ ] docker-compose.yml completo
- [ ] docker-compose.override.yml (dev)
- [ ] docker-compose.prod.yml
- [ ] Secrets management
- [ ] Health checks

---

## 7.3 Kubernetes (Opcional)

### Estrutura
```
k8s/
â”œâ”€â”€ base/
â”‚   â”œâ”€â”€ namespace.yaml
â”‚   â”œâ”€â”€ backend-deployment.yaml
â”‚   â”œâ”€â”€ backend-service.yaml
â”‚   â”œâ”€â”€ frontend-deployment.yaml
â”‚   â”œâ”€â”€ frontend-service.yaml
â”‚   â”œâ”€â”€ ingress.yaml
â”‚   â””â”€â”€ kustomization.yaml
â”œâ”€â”€ overlays/
â”‚   â”œâ”€â”€ dev/
â”‚   â””â”€â”€ prod/
â””â”€â”€ secrets/
    â””â”€â”€ sealed-secrets.yaml
```

### EntregÃ¡veis
- [ ] Deployments
- [ ] Services
- [ ] Ingress
- [ ] ConfigMaps
- [ ] Secrets (sealed)
- [ ] HPA (autoscaling)
- [ ] PDB (disruption budget)

---

## 7.4 CD Pipeline

```yaml
# .github/workflows/cd.yml
name: CD

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Build and push Docker images
        run: |
          docker build -t registry/etl-backend:${{ github.sha }} ./backend
          docker build -t registry/etl-frontend:${{ github.sha }} ./frontend
          docker push registry/etl-backend:${{ github.sha }}
          docker push registry/etl-frontend:${{ github.sha }}

      - name: Deploy to Kubernetes
        run: |
          kubectl set image deployment/backend backend=registry/etl-backend:${{ github.sha }}
          kubectl set image deployment/frontend frontend=registry/etl-frontend:${{ github.sha }}
```

### EntregÃ¡veis
- [ ] CI/CD completo
- [ ] Deploy automÃ¡tico
- [ ] Rollback automÃ¡tico
- [ ] Canary/Blue-Green

---

# FASE 8: DocumentaÃ§Ã£o ğŸ“š

## 8.1 DocumentaÃ§Ã£o TÃ©cnica

### Estrutura
```
docs/
â”œâ”€â”€ README.md                    # VisÃ£o geral
â”œâ”€â”€ ARCHITECTURE.md              # Arquitetura detalhada
â”œâ”€â”€ API.md                       # ReferÃªncia da API
â”œâ”€â”€ DEPLOYMENT.md                # Guia de deploy
â”œâ”€â”€ DEVELOPMENT.md               # Guia de desenvolvimento
â”œâ”€â”€ SECURITY.md                  # PrÃ¡ticas de seguranÃ§a
â”œâ”€â”€ TROUBLESHOOTING.md           # Problemas comuns
â””â”€â”€ CHANGELOG.md                 # HistÃ³rico de mudanÃ§as
```

### EntregÃ¡veis
- [ ] README atualizado
- [ ] Arquitetura documentada
- [ ] API documentada (OpenAPI)
- [ ] Guia de deploy
- [ ] Runbook operacional

---

## 8.2 API Documentation (OpenAPI)

```python
# backend/app.py
app = FastAPI(
    title="ETL Dashboard API",
    description="API para gerenciamento de pipelines ETL",
    version="2.0.0",
    docs_url="/api/docs",
    redoc_url="/api/redoc",
)
```

### EntregÃ¡veis
- [ ] Swagger UI (`/api/docs`)
- [ ] ReDoc (`/api/redoc`)
- [ ] Exemplos de request/response
- [ ] AutenticaÃ§Ã£o documentada

---

## 8.3 ADRs (Architecture Decision Records)

```
docs/adr/
â”œâ”€â”€ 001-use-fastapi.md
â”œâ”€â”€ 002-postgresql-over-sqlite.md
â”œâ”€â”€ 003-celery-for-jobs.md
â”œâ”€â”€ 004-jwt-authentication.md
â””â”€â”€ template.md
```

### Template ADR
```markdown
# ADR-XXX: TÃ­tulo

## Status
Aceito | Rejeitado | SubstituÃ­do

## Contexto
Por que essa decisÃ£o foi necessÃ¡ria?

## DecisÃ£o
O que foi decidido?

## ConsequÃªncias
Quais os impactos positivos e negativos?
```

### EntregÃ¡veis
- [ ] ADRs para decisÃµes principais
- [ ] Template de ADR
- [ ] Processo de revisÃ£o

---

# ğŸ“‹ Checklist Final

## SeguranÃ§a
- [ ] AutenticaÃ§Ã£o JWT implementada
- [ ] Credenciais criptografadas
- [ ] ValidaÃ§Ã£o de entrada
- [ ] Rate limiting
- [ ] Headers de seguranÃ§a
- [ ] CORS restritivo

## Banco de Dados
- [ ] PostgreSQL configurado
- [ ] SQLAlchemy ORM
- [ ] MigraÃ§Ãµes Alembic
- [ ] Ãndices otimizados

## Arquitetura
- [ ] Dependency Injection
- [ ] ConfiguraÃ§Ã£o centralizada
- [ ] Estado gerenciado
- [ ] Logs separados

## Qualidade
- [ ] 80%+ cobertura backend
- [ ] 70%+ cobertura frontend
- [ ] CI/CD configurado
- [ ] Linting + formataÃ§Ã£o

## Escalabilidade
- [ ] Celery para jobs
- [ ] Redis pub/sub
- [ ] Cache implementado
- [ ] MÃºltiplos workers

## Observabilidade
- [ ] Logging estruturado
- [ ] MÃ©tricas Prometheus
- [ ] Health checks
- [ ] Tracing (opcional)

## DevOps
- [ ] Docker images
- [ ] Docker Compose
- [ ] Kubernetes (opcional)
- [ ] Deploy automÃ¡tico

## DocumentaÃ§Ã£o
- [ ] README completo
- [ ] API documentada
- [ ] Guia de deploy
- [ ] ADRs

---

# ğŸ—“ï¸ Cronograma Sugerido

```
Semana 1-2:  FASE 1 - SeguranÃ§a
Semana 3:    FASE 2 - Banco de Dados
Semana 4-5:  FASE 3 - Arquitetura
Semana 6-7:  FASE 4 - Qualidade (Testes)
Semana 8-9:  FASE 5 - Escalabilidade
Semana 10:   FASE 6 - Observabilidade
Semana 11:   FASE 7 - DevOps
Semana 12:   FASE 8 - DocumentaÃ§Ã£o + Buffer
```

---

**Nota**: Este plano Ã© iterativo. Cada fase pode ser ajustada conforme necessidades especÃ­ficas do projeto. Priorize sempre seguranÃ§a e qualidade sobre features adicionais.
